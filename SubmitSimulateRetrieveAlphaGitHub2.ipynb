{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54221cc-7633-459e-b29d-4076b6818b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "import time\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# Configure logging to output to both console and a log file\n",
    "log_filename = \"simulation_log.txt\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, mode='w'),  # Write logs to a file\n",
    "        logging.StreamHandler()  # Also print logs to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "brain_api_url = os.environ.get(\"BRAIN_API_URL\", \"https://api.worldquantbrain.com\")\n",
    "brain_url = os.environ.get(\"BRAIN_URL\", \"https://platform.worldquantbrain.com\") \n",
    "\n",
    "class RateLimitExceededError(Exception):\n",
    "    \"\"\"Custom exception for rate limit errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class BRAINAPIWRAPPER:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = self.get_login_session()\n",
    "        self.permissions = self.check_permissions()  # Initialize permissions here\n",
    "\n",
    "        \n",
    "    def get_login_session(self):\n",
    "        session = requests.Session()\n",
    "        username = os.getenv('wqbrain_consultant_user')\n",
    "        password = os.getenv('wqbrain_consultant_pw')\n",
    "        session.auth = (username, password)\n",
    "        response = session.post('https://api.worldquantbrain.com/authentication')\n",
    "        response.headers\n",
    "        print(username)\n",
    "\n",
    "        if response.status_code == requests.status_codes.codes.unauthorized:\n",
    "            if response.headers[\"WWW-Authenticate\"] == \"persona\":\n",
    "                biometric_url = urljoin(response.url, response.headers[\"Location\"])\n",
    "                print(biometric_url)\n",
    "                input(\"Complete bio\" + biometric_url)\n",
    "                biometric_response = session.post(biometric_url)\n",
    "        else:\n",
    "            print(\"incorrect\")\n",
    "        return session\n",
    "    \n",
    "    def check_permissions(self):\n",
    "        response = self.session.get('https://api.worldquantbrain.com/authentication')\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            permissions = data.get('permissions', [])\n",
    "            print(\"User permissions: \", permissions)\n",
    "            return permissions\n",
    "        else:\n",
    "            print(\"Failed to retrieve permissions: \", response.status_code)\n",
    "            return []\n",
    "    \n",
    "    def has_multi_simulation_permission(self):\n",
    "        return \"MULTI_SIMULATION\" in self.permissions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b3b75-2064-457d-b791-457965eedb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = BRAINAPIWRAPPER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb0163-f12c-4bb7-af73-b5eb466eafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_type(self, item):\n",
    "    # Check item metadata or structure to identify type\n",
    "    # This is an example; adapt based on your data structure.\n",
    "    if hasattr(item, 'data_type'):\n",
    "        return item.data_type  # Assume each item has a 'data_type' attribute\n",
    "    # Or use other logic as needed\n",
    "    return 'unknown'\n",
    "\n",
    "BRAINAPIWRAPPER.get_data_type = get_data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d6171-582d-4760-90e6-8589bd3a8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_first_column_from_csv(filename):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and returns the first column as a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the first column of the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Type of filename: {type(filename)}\")  # This should print <class 'str'>\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        # Load the first column into a new DataFrame\n",
    "        first_column_df = df.iloc[:, [0]]  # iloc[:, [0]] selects the first column\n",
    "\n",
    "        return first_column_df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "BRAINAPIWRAPPER.load_first_column_from_csv = load_first_column_from_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551be4c-159d-41e5-b716-441f5c75d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_first_column_with_type_check(filename):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and returns the first column, filtering only 'vector' entries if they exist.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing only rows where the first column type is 'vector'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the entire CSV file into a DataFrame\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Assume the second column contains type information (e.g., 'vector' or 'matrix')\n",
    "        if 'type' in df.columns:\n",
    "            # Filter for rows where 'type' is 'vector'\n",
    "            filtered_df = df[df['type'] == 'matrix'].iloc[:, [0]]\n",
    "        else:\n",
    "            print(\"No 'type' column found. Returning first column without filtering.\")\n",
    "            filtered_df = df.iloc[:, [0]]  # No filtering, only first column\n",
    "\n",
    "        return filtered_df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "BRAINAPIWRAPPER.load_first_column_with_type_check = load_first_column_with_type_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad655250-19d5-432f-ab68-0fe8cb712439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_print_first_column_with_matrix_type(filename):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, prints each row's type, and returns only rows where type is 'matrix'.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the first column for rows where type is 'matrix'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the entire CSV file into a DataFrame\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        # Check if there's a 'type' column\n",
    "        if 'type' in df.columns:\n",
    "            # Initialize an empty list to store rows with 'matrix' type\n",
    "            matrix_rows = []\n",
    "\n",
    "            # Iterate over each row and print the type\n",
    "            for index, row in df.iterrows():\n",
    "                row_type = row['type']  # Get the type from the 'type' column\n",
    "                first_column_value = row.iloc[0]  # Get the value in the first column\n",
    "\n",
    "                # Print the row type and first column value for inspection, comment out as too long\n",
    "                # print(f\"Row {index}: Type = {row_type}, First Column Value = {first_column_value}\")\n",
    "\n",
    "                # Only add rows with 'matrix' type to the list\n",
    "                if row_type == 'MATRIX':\n",
    "                    matrix_rows.append(row)\n",
    "\n",
    "            # Create a DataFrame from the filtered rows\n",
    "            filtered_df = pd.DataFrame(matrix_rows)\n",
    "            # Return only the first column from the filtered DataFrame\n",
    "            return filtered_df.iloc[:, [0]]\n",
    "        \n",
    "        else:\n",
    "            print(\"No 'type' column found. Returning the first column without filtering.\")\n",
    "            # No filtering; return the first column as-is\n",
    "            return df.iloc[:, [0]]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "BRAINAPIWRAPPER.load_and_print_first_column_with_matrix_type = load_and_print_first_column_with_matrix_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3710bdd-0bd3-4808-8f4f-513c26a56fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_print_first_column_with_matrix_type2(filename):\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(filename)\n",
    "        print(\"CSV File Loaded Successfully. Preview:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Ensure the 'type' column exists (case insensitive check)\n",
    "        if 'type' not in df.columns:\n",
    "            print(\"No 'type' column found. Returning the first column without filtering.\")\n",
    "            return df.iloc[:, [0]]\n",
    "\n",
    "        # Debug: Print the entire 'type' column\n",
    "        print(\"Type column values:\")\n",
    "        print(df['type'])\n",
    "\n",
    "        # Initialize an empty list to store rows with 'MATRIX' type\n",
    "        matrix_rows = []\n",
    "\n",
    "        # Iterate over each row and print its details\n",
    "        for index, row in df.iterrows():\n",
    "            # Debug: Inspect the row\n",
    "            print(f\"Debugging Row {index}: {row}\")\n",
    "\n",
    "            # Check if 'type' is valid\n",
    "            if pd.isna(row['type']) or not isinstance(row['type'], str):\n",
    "                print(f\"Skipping Row {index}: Invalid 'type' value = {row['type']}\")\n",
    "                continue\n",
    "\n",
    "            # Normalize the 'type' column value for comparison\n",
    "            row_type = str(row['type']).strip().upper()\n",
    "            first_column_value = row.iloc[0]\n",
    "\n",
    "            # Debug: Print details of the current row , commented out as too long\n",
    "            # print(f\"Row {index}: Type = {row_type}, First Column Value = {first_column_value}\")\n",
    "\n",
    "            # Add rows with 'MATRIX' type to the list\n",
    "            if row_type == 'MATRIX':\n",
    "                matrix_rows.append(row)\n",
    "\n",
    "        # Create a DataFrame from the filtered rows\n",
    "        filtered_df = pd.DataFrame(matrix_rows)\n",
    "\n",
    "        # Return only the first column from the filtered DataFrame\n",
    "        return filtered_df.iloc[:, [0]]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "BRAINAPIWRAPPER.load_and_print_first_column_with_matrix_type2 = load_and_print_first_column_with_matrix_type2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07001d5a-d147-482d-b2a6-c2b0664d2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_print_first_column_with_vector_type(filename):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, prints each row's type, and returns only rows where type is 'vector'.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the first column for rows where type is 'matrix'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the entire CSV file into a DataFrame\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        # Check if there's a 'type' column\n",
    "        if 'type' in df.columns:\n",
    "            # Initialize an empty list to store rows with 'matrix' type\n",
    "            vector_rows = []\n",
    "\n",
    "            # Iterate over each row and print the type\n",
    "            for index, row in df.iterrows():\n",
    "                row_type = row['type']  # Get the type from the 'type' column\n",
    "                first_column_value = row.iloc[0]  # Get the value in the first column\n",
    "\n",
    "                # Print the row type and first column value for inspection, comment out as too long\n",
    "                # print(f\"Row {index}: Type = {row_type}, First Column Value = {first_column_value}\")\n",
    "\n",
    "                # Only add rows with 'matrix' type to the list\n",
    "                if row_type == 'VECTOR':\n",
    "                    matrix_rows.append(row)\n",
    "\n",
    "            # Create a DataFrame from the filtered rows\n",
    "            filtered_df = pd.DataFrame(vector_rows)\n",
    "            # Return only the first column from the filtered DataFrame\n",
    "            return filtered_df.iloc[:, [0]]\n",
    "        \n",
    "        else:\n",
    "            print(\"No 'type' column found. Returning the first column without filtering.\")\n",
    "            # No filtering; return the first column as-is\n",
    "            return df.iloc[:, [0]]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "BRAINAPIWRAPPER.load_and_print_first_column_with_vector_type = load_and_print_first_column_with_vector_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39daca20-fcd9-4d12-af9b-c2752a07322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIRED\n",
    "def create_simulation_data(self, first_column):\n",
    "    \"\"\"\n",
    "    Creates a simulation data dictionary with the first column.\n",
    "\n",
    "    Parameters:\n",
    "    first_column (list): The first column data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A simulation data dictionary.\n",
    "    \"\"\"\n",
    "    # Convert the first column list to a string format for the 'regular' field\n",
    "    first_column_str = ', '.join(map(str, first_column))  # Convert each item to string and join\n",
    "\n",
    "    simulation_data = {\n",
    "        'type': 'REGULAR',\n",
    "        'settings': {\n",
    "            'instrumentType': 'EQUITY',\n",
    "            'region':'ASI',\n",
    "            'universe': 'MINVOL1M',\n",
    "            'delay': 1,\n",
    "            'decay' : 4,\n",
    "            'neutralization': 'NONE',\n",
    "            'truncation': 0.08,\n",
    "            'pasteurization': 'ON',\n",
    "            'testPeriod': 'P6Y0M0D',\n",
    "            'unitHandling': 'VERIFY',\n",
    "            'nanHandling': 'OFF',\n",
    "            'language': 'FASTEXPR',\n",
    "            'visualization': False,\n",
    "        },\n",
    "        'regular': f'ts_rank(({first_column_str}), 20)'  # Format the string\n",
    "        #'regular': f'ts_decay_linear({first_column_str}, 10) * rank(volume*close) + ts_decay_linear({first_column_str}, 50) * (1 - rank(volume*close))'\n",
    "    }\n",
    "\n",
    "    return simulation_data\n",
    "\n",
    "\n",
    "BRAINAPIWRAPPER.create_simulation_data = create_simulation_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a1a37-d0d6-462b-952f-8782b3340db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIRED\n",
    "def send_simulation(self, simulation_settings):\n",
    "\n",
    "    # Check if simulation_settings is a dictionary\n",
    "    if not isinstance(simulation_settings, dict):\n",
    "        raise ValueError(\"simulation_settings must be a dictionary.\")\n",
    "    \n",
    "    simulation_response = self.session.post ('https://api.worldquantbrain.com/simulations',json=simulation_settings)\n",
    "#    simulation_response = self.session.post ('https://api.worldquantbrain.com/simulations',json=truncated_alpha_list2)\n",
    "\n",
    "    #Comment out as printout too long\n",
    "    #print(simulation_response.headers)\n",
    "    print(simulation_response.text)\n",
    "    \n",
    "    location = simulation_response.headers.get('Location')\n",
    "    if location is not None:\n",
    "        print(\"Location: \" + location)\n",
    "    else:\n",
    "        print(\"Warning: 'Location' header not found in the response. Continuing to the next simulation.\")\n",
    "        return None  # Return None or handle as needed, but continue processing\n",
    "    simulation_location = simulation_response.headers['Location']\n",
    "\n",
    "    return simulation_location\n",
    "\n",
    "#original code simulation_progress_url changed to simulation_location\n",
    "BRAINAPIWRAPPER.send_simulation = send_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a152bbe-cfc6-476f-9c5c-8b47822cd9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_location_response = s.send_simulation(simulation_data)\n",
    "print(\"Location Response: \" + simulation_location_response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b68a6c-30b4-4fd8-907f-dbb3f20b28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD\n",
    "def check_progress_return_alpha(self, simulation_location_response):\n",
    "\n",
    "    while True:\n",
    "        simulation_progress = self.session.get(simulation_location_response)\n",
    "\n",
    "        # comment out as printout too long\n",
    "        # print(\"Simulation Progress Response:\", simulation_progress.json())\n",
    "        \n",
    "        if simulation_progress.headers.get(\"Retry-After\",0) == 0:\n",
    "            break\n",
    "        #Commented as printout is too long\n",
    "        #print (\"Sleeping for \" + simulation_progress.headers[\"Retry-After\"] + \" secs.\")\n",
    "        sleep(float(simulation_progress.headers[\"Retry-After\"]))\n",
    "    json_data = simulation_progress.json()\n",
    "#    print(\"Simulation progress JSON:\", json_data)        # HERE\n",
    "\n",
    "    try:\n",
    "        alpha_id = json_data[\"alpha\"]\n",
    "    except KeyError:\n",
    "        print(\"Error: 'alpha' key not found in the response.\")\n",
    "        return None  # or handle it in another way, e.g., raise an exception\n",
    "\n",
    "BRAINAPIWRAPPER.check_progress_return_alpha = check_progress_return_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac076d-38ff-4fb6-9f2a-bf4b45069dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REVISED 20241027\n",
    "#REQUIRED\n",
    "def check_progress_return_alpha(self, simulation_location_response):\n",
    "\n",
    "    while True:\n",
    "        simulation_progress = self.session.get(simulation_location_response)\n",
    "        json_data = simulation_progress.json()\n",
    "        \n",
    "        # comment out as printout too long\n",
    "        # print(\"Simulation Progress Response:\", json_data)\n",
    "\n",
    "        # Check if the simulation is complete\n",
    "        if json_data.get(\"status\") == \"COMPLETE\":\n",
    "            try:\n",
    "                alpha_id = json_data[\"alpha\"]\n",
    "                print(f\"Check Progress Returned alpha_id: {alpha_id}\")  # Debug: Confirm alpha_id retrieved\n",
    "                return alpha_id  # Return the alpha_id if the simulation is complete\n",
    "            except KeyError:\n",
    "                print(\"ERROR: 'alpha' key not found in the response.\")\n",
    "                return None\n",
    "\n",
    "        # Handle Retry-After for ongoing simulation\n",
    "        retry_after = simulation_progress.headers.get(\"Retry-After\")\n",
    "        if retry_after:\n",
    "            # Commented out as printout is too long\n",
    "            # print(f\"Sleeping for {retry_after} secs.\")\n",
    "            sleep(float(retry_after))\n",
    "        else:\n",
    "            print(\"No Retry-After header found; waiting 2 seconds by default.\")\n",
    "            sleep(2)  # Default delay if no \"Retry-After\" header is provided\n",
    "\n",
    "BRAINAPIWRAPPER.check_progress_return_alpha = check_progress_return_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136aae6-f6fc-49f3-97ef-dadb58eac1ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_response = s.check_progress_return_alpha(simulation_location_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae586d-7fe8-4b8b-8ec1-691f61f48879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIRED\n",
    "\n",
    "def simulate_alpha(self, simulation_settings):\n",
    "    simulation_location = self.send_simulation(simulation_settings)\n",
    "    alpha_id = self.check_progress_return_alpha(simulation_location)\n",
    "    simulation_results = self.get_results_dictionary(alpha_id)\n",
    "    return simulation_results\n",
    "\n",
    "BRAINAPIWRAPPER.simulate_alpha = simulate_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931fb49c-962e-4536-8f6b-104677394bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIRED\n",
    "\n",
    "def get_results_dictionary(self, alpha_id):\n",
    "# Check if alpha_id is valid\n",
    "    if not alpha_id:  # This checks for None, empty string, or any falsy value\n",
    "        print(\"Get results dict Error: Invalid alpha_id. Please provide a valid alpha_id.\")\n",
    "        return None  # Or raise an exception, depending on your error handling strategy\n",
    "\n",
    "    simulation_results = {\n",
    "        'summary_results aka get_results_dictionary': self.get_results(alpha_id),\n",
    "        'submission_checks aka get_results_dictionary': self.get_results(alpha_id, stats=\"/check\")\n",
    "    }\n",
    "\n",
    "    # commenting as logging too long\n",
    "    # print(simulation_results)\n",
    "\n",
    "    #print to file\n",
    "    logging.info(f\"simu_results: {simulation_results}\\n\\n\")\n",
    "    \n",
    "    return simulation_results\n",
    "\n",
    "BRAINAPIWRAPPER.get_results_dictionary = get_results_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41584c7a-9ef6-47be-994d-e03d0639d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(self, simulation_results):\n",
    "\n",
    "    # Convert the results to a JSON string\n",
    "    dict_string2 = json.dumps(simulation_results, ensure_ascii=False)\n",
    "    \n",
    "    # Print the modified string for debugging\n",
    "    print(\"dict_string: \" + dict_string2)\n",
    "    \n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    filename = f'datasets_{current_date}.txt'\n",
    "    \n",
    "    # Open the file in append mode\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(dict_string2 + \"\\n\\n\")  # Append the results and add a newline for separation\n",
    "\n",
    "    return simulation_results\n",
    "\n",
    "BRAINAPIWRAPPER.save_results = save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a81f3-738f-4945-b628-36e062c520c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIRED\n",
    "\n",
    "def get_results(self, alpha_id, stats=\"\"):\n",
    "    results = self.session.get(\"https://api.worldquantbrain.com/alphas/\" + alpha_id + stats)\n",
    "#    print(\"results: \" + results)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "    print(\"get_results: \" + results.text, \"\\n\\n\")  # Use .text or .json() to print the response content    \n",
    "\n",
    "    #print to file\n",
    "    logging.info(f\"get_results: {results.text}\\n\\n\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    return results.json()\n",
    "BRAINAPIWRAPPER.get_results = get_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6db60a-3869-4464-bad4-61a44d8e0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(self, alpha_id, stats=\"\"):\n",
    "    try:\n",
    "        # Make the API request\n",
    "        results = self.session.get(f\"https://api.worldquantbrain.com/alphas/{alpha_id}{stats}\")\n",
    "\n",
    "        # Check for rate limit error in the response\n",
    "        if \"No Retry-After header found\" in results.text or results.status_code == 429:\n",
    "            logging.error(\"Rate limit exceeded. Stopping further processing.\")\n",
    "            raise RateLimitExceededError(\"Rate limit exceeded. API requests halted.\")\n",
    "\n",
    "        # Print and log the results\n",
    "        # print(f\"get_results: {results.text}\\n\\n\")\n",
    "        logging.info(f\"get_results: {results.text}\\n\\n\")\n",
    "\n",
    "        # Return the JSON response\n",
    "        return results.json()\n",
    "    \n",
    "    except RateLimitExceededError:\n",
    "        raise  # Re-raise the custom exception for higher-level handling\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in get_results: {e}\")\n",
    "\n",
    "BRAINAPIWRAPPER.get_results = get_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58468c1-383a-44f6-8b22-e92cd8b033d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main batch processing function\n",
    "def process_simulations_in_batches(filename, batch_size=3):\n",
    "\n",
    "    # Print type to confirm\n",
    "    print(f\"In process_simulations_in_batches, type of filename: {type(filename)}\")\n",
    "\n",
    "    # Verify filename is a string\n",
    "    if not isinstance(filename, str):\n",
    "        print(\"Error: filename must be a string representing the file path.\")\n",
    "        return\n",
    "    \n",
    "    # Step 1: Load the first column of data from the CSV file\n",
    "    # first_column_df = load_first_column_from_csv(filename)\n",
    "    first_column_df = load_first_column_with_type_check(filename)\n",
    "    if first_column_df is None:\n",
    "        print(\"Failed to load data from CSV.\")\n",
    "        return\n",
    "\n",
    "    first_column_list = first_column_df.iloc[:, 0].tolist()  # Convert to list\n",
    "\n",
    "    # Step 2: Process in batches\n",
    "    results = []\n",
    "    for i in range(0, len(first_column_list), batch_size):\n",
    "        batch = first_column_list[i:i + batch_size]\n",
    "        \n",
    "        for alpha in batch:\n",
    "            # Step 3: Create simulation data for each alpha\n",
    "            print(\"current expression: \" + alpha)        \n",
    "            simulation_settings = s.create_simulation_data([alpha])\n",
    "            \n",
    "            # Step 4: Send the simulation and check for progress\n",
    "            simulation_location = s.send_simulation(simulation_settings)\n",
    "            if simulation_location is None:\n",
    "                print(\"Simulation initiation failed, moving to the next item.\")\n",
    "                continue  # Skip to the next item in the batch if initiation failed\n",
    "            \n",
    "            # Step 5: Monitor progress and retrieve alpha ID\n",
    "            alpha_id = s.check_progress_return_alpha(simulation_location)\n",
    "            if alpha_id is None:\n",
    "                print(f\"Failed to retrieve alpha_id for {alpha}.\")\n",
    "                continue\n",
    "            \n",
    "            # Step 6: Get results for the completed simulation\n",
    "            simulation_result = s.get_results_dictionary(alpha_id)\n",
    "            results.append({alpha: simulation_result})\n",
    "            print(f\"Number of alphas returned so far: {len(results)}\")\n",
    "            \n",
    "            # Optional: small delay between individual simulations\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "        \n",
    "        print(f\"Processed batch {(i // batch_size) + 1} of {len(first_column_list) // batch_size + 1}\")\n",
    "\n",
    "        # Step 7: Delay between batches to manage API load\n",
    "        time.sleep(random.uniform(1, 2))  # Adjust as necessary for API rate limits\n",
    "\n",
    "    return results\n",
    "\n",
    "BRAINAPIWRAPPER.process_simulations_in_batches = process_simulations_in_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f22811-c716-4194-b63d-53677ef711fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main batch processing function\n",
    "def process_simulations_in_batches2(filename, batch_size=3):\n",
    "    print(f\"In process_simulations_in_batches, type of filename: {type(filename)}\")\n",
    "\n",
    "    if not isinstance(filename, str):\n",
    "        print(\"Error: filename must be a string representing the file path.\")\n",
    "        return\n",
    "    \n",
    "    # first_column_df = load_first_column_from_csv(filename)\n",
    "    first_column_df = load_and_print_first_column_with_matrix_type(filename)\n",
    "#    first_column_df = load_and_print_first_column_with_vector_type(filename)\n",
    "    if first_column_df is None:\n",
    "        print(\"Failed to load data from CSV.\")\n",
    "        return\n",
    "\n",
    "    first_column_list = first_column_df.iloc[:, 0].tolist()\n",
    "\n",
    "    # Store all results that meet the criteria\n",
    "    all_results = []\n",
    "    for i in range(0, len(first_column_list), batch_size):\n",
    "        batch = first_column_list[i:i + batch_size]\n",
    "        \n",
    "        current_batch_results = []\n",
    "\n",
    "        for alpha in batch:\n",
    "            print(\"Current expression: \" + alpha)        \n",
    "            simulation_settings = s.create_simulation_data([alpha])\n",
    "            \n",
    "            simulation_location = s.send_simulation(simulation_settings)\n",
    "            if simulation_location is None:\n",
    "                print(\"Simulation initiation failed, moving to the next item.\")\n",
    "                continue\n",
    "            \n",
    "            alpha_id = s.check_progress_return_alpha(simulation_location)\n",
    "            if alpha_id is None:\n",
    "                print(f\"Failed to retrieve alpha_id for {alpha}.\")\n",
    "                continue\n",
    "            \n",
    "            simulation_result = s.get_results_dictionary(alpha_id)\n",
    "\n",
    "            sharpe_ratio = simulation_result.get(\"sharpe_ratio\", None)\n",
    "            if sharpe_ratio is not None and sharpe_ratio > 0.7:\n",
    "                current_batch_results.append({\n",
    "                    \"alpha_id\": alpha_id,\n",
    "                    \"expression\": alpha,\n",
    "                    \"sharpe_ratio\": sharpe_ratio,\n",
    "                    \"universe\": simulation_result.get(\"universe\"),\n",
    "                    \"region\": simulation_result.get(\"region\"),\n",
    "                    \"turnover\": simulation_result.get(\"turnover\"),\n",
    "                    \"drawdown\": simulation_result.get(\"drawdown\"),\n",
    "                    \"fitness\": simulation_result.get(\"fitness\"),\n",
    "                })\n",
    "                # Append valid results to all_results\n",
    "                all_results.append({\n",
    "                    \"alpha_id\": alpha_id,\n",
    "                    \"expression\": alpha,\n",
    "                    \"sharpe_ratio\": sharpe_ratio,\n",
    "                    \"universe\": simulation_result.get(\"universe\"),\n",
    "                    \"region\": simulation_result.get(\"region\"),\n",
    "                    \"turnover\": simulation_result.get(\"turnover\"),\n",
    "                    \"drawdown\": simulation_result.get(\"drawdown\"),\n",
    "                    \"fitness\": simulation_result.get(\"fitness\"),\n",
    "                })\n",
    "                print(f\"Sharpe Ratio {sharpe_ratio} is greater than 1.1 for alpha: {alpha}\")\n",
    "\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "        \n",
    "        if current_batch_results:\n",
    "            output_filename = f\"batch_results_{i // batch_size + 1}.csv\"  \n",
    "            with open(output_filename, mode='w', newline='') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=current_batch_results[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(current_batch_results)\n",
    "\n",
    "            print(f\"Batch results written to {output_filename}\")\n",
    "        else:\n",
    "            print(f\"No results to write for batch {i // batch_size + 1}.\")\n",
    "\n",
    "        print(f\"Processed batch {(i // batch_size) + 1} of {len(first_column_list) // batch_size + 1}\")\n",
    "\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    # Save all_results with the date in the filename\n",
    "    if all_results:\n",
    "        today_date = datetime.now().strftime(\"%Y%m%d\")  # Get today's date in YYYYMMDD format\n",
    "        all_results_filename = f\"{today_date}.csv\"  # Create the filename with today's date\n",
    "        with open(all_results_filename, mode='w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=all_results[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_results)\n",
    "        print(f\"All valid results saved to {all_results_filename}\")\n",
    "\n",
    "BRAINAPIWRAPPER.process_simulations_in_batches2 = process_simulations_in_batches2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8150aab-7156-4c24-b2b5-ef546e788f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_simulations_in_batches3(filename, batch_size=3):\n",
    "    logging.info(f\"In process_simulations_in_batches, type of filename: {type(filename)}\")\n",
    "\n",
    "    if not isinstance(filename, str):\n",
    "        logging.error(\"Error: filename must be a string representing the file path.\")\n",
    "        return\n",
    "    \n",
    "    # first_column_df = load_first_column_from_csv(filename)\n",
    "    first_column_df = load_and_print_first_column_with_matrix_type(filename)\n",
    "    #    first_column_df = load_and_print_first_column_with_vector_type(filename)\n",
    "    if first_column_df is None:\n",
    "        logging.error(\"Failed to load data from CSV.\")\n",
    "        return\n",
    "\n",
    "    first_column_list = first_column_df.iloc[:, 0].tolist()\n",
    "\n",
    "    # Store all results that meet the criteria\n",
    "    all_results = []\n",
    "    for i in range(0, len(first_column_list), batch_size):\n",
    "        batch = first_column_list[i:i + batch_size]\n",
    "        \n",
    "        current_batch_results = []\n",
    "\n",
    "        for alpha in batch:\n",
    "            logging.info(f\"Current expression: {alpha}\")        \n",
    "            simulation_settings = s.create_simulation_data([alpha])\n",
    "            \n",
    "            simulation_location = s.send_simulation(simulation_settings)\n",
    "            if simulation_location is None:\n",
    "                logging.warning(\"Simulation initiation failed, moving to the next item.\")\n",
    "                continue\n",
    "            \n",
    "            alpha_id = s.check_progress_return_alpha(simulation_location)\n",
    "            if alpha_id is None:\n",
    "                logging.warning(f\"Failed to retrieve alpha_id for {alpha}.\")\n",
    "                continue\n",
    "            \n",
    "            simulation_result = s.get_results_dictionary(alpha_id)\n",
    "\n",
    "            sharpe_ratio = simulation_result.get(\"sharpe_ratio\", None)\n",
    "            if sharpe_ratio is not None and sharpe_ratio > 0.7:\n",
    "                current_batch_results.append({\n",
    "                    \"alpha_id\": alpha_id,\n",
    "                    \"expression\": alpha,\n",
    "                    \"sharpe_ratio\": sharpe_ratio,\n",
    "                    \"universe\": simulation_result.get(\"universe\"),\n",
    "                    \"region\": simulation_result.get(\"region\"),\n",
    "                    \"turnover\": simulation_result.get(\"turnover\"),\n",
    "                    \"drawdown\": simulation_result.get(\"drawdown\"),\n",
    "                    \"fitness\": simulation_result.get(\"fitness\"),\n",
    "                })\n",
    "                # Append valid results to all_results\n",
    "                all_results.append({\n",
    "                    \"alpha_id\": alpha_id,\n",
    "                    \"expression\": alpha,\n",
    "                    \"sharpe_ratio\": sharpe_ratio,\n",
    "                    \"universe\": simulation_result.get(\"universe\"),\n",
    "                    \"region\": simulation_result.get(\"region\"),\n",
    "                    \"turnover\": simulation_result.get(\"turnover\"),\n",
    "                    \"drawdown\": simulation_result.get(\"drawdown\"),\n",
    "                    \"fitness\": simulation_result.get(\"fitness\"),\n",
    "                })\n",
    "                logging.info(f\"Sharpe Ratio {sharpe_ratio} is greater than 0.7 for alpha: {alpha}\")\n",
    "\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "        \n",
    "        if current_batch_results:\n",
    "            output_filename = f\"batch_results_{i // batch_size + 1}.csv\"  \n",
    "            with open(output_filename, mode='w', newline='') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=current_batch_results[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(current_batch_results)\n",
    "\n",
    "            logging.info(f\"Batch results written to {output_filename}\")\n",
    "        else:\n",
    "            logging.info(f\"No results to write for batch {i // batch_size + 1}.\")\n",
    "\n",
    "        logging.info(f\"Processed batch {(i // batch_size) + 1} of {len(first_column_list) // batch_size + 1}\")\n",
    "\n",
    "        # Save logs before hitting rate limits\n",
    "        logging.info(f\"Pausing to handle rate limit.\")\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    # Save all_results with the date in the filename\n",
    "    if all_results:\n",
    "        today_date = datetime.now().strftime(\"%Y%m%d\")  # Get today's date in YYYYMMDD format\n",
    "        all_results_filename = f\"{today_date}_all_results.csv\"  # Create the filename with today's date\n",
    "        with open(all_results_filename, mode='w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=all_results[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_results)\n",
    "        logging.info(f\"All valid results saved to {all_results_filename}\")\n",
    "\n",
    "BRAINAPIWRAPPER.process_simulations_in_batches3 = process_simulations_in_batches3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d9741-a324-41b1-a72c-dcfbaee81934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_simulations_in_batches4(filename, batch_size=3):\n",
    "    logging.info(f\"Processing simulations in batches. Filename type: {type(filename)}\")\n",
    "\n",
    "    if not isinstance(filename, str):\n",
    "        logging.error(\"Error: filename must be a string representing the file path.\")\n",
    "        return\n",
    "    \n",
    "    first_column_df = load_and_print_first_column_with_matrix_type(filename)\n",
    "    if first_column_df is None:\n",
    "        logging.error(\"Failed to load data from CSV.\")\n",
    "        return\n",
    "\n",
    "    first_column_list = first_column_df.iloc[:, 0].tolist()\n",
    "\n",
    "    all_results = []\n",
    "    for i in range(0, len(first_column_list), batch_size):\n",
    "        batch = first_column_list[i:i + batch_size]\n",
    "        logging.info(f\"Processing batch {i // batch_size + 1} with {len(batch)} items.\")\n",
    "\n",
    "        current_batch_results = []\n",
    "        for alpha in batch:\n",
    "            logging.info(f\"Processing expression: {alpha}\")\n",
    "            try:\n",
    "                simulation_settings = s.create_simulation_data([alpha])\n",
    "                simulation_location = s.send_simulation(simulation_settings)\n",
    "\n",
    "                if simulation_location is None:\n",
    "                    logging.warning(\"Simulation initiation failed, skipping this expression.\")\n",
    "                    continue\n",
    "                \n",
    "                alpha_id = s.check_progress_return_alpha(simulation_location)\n",
    "                if alpha_id is None:\n",
    "                    logging.warning(f\"Failed to retrieve alpha_id for expression: {alpha}.\")\n",
    "                    continue\n",
    "\n",
    "                simulation_result = s.get_results_dictionary(alpha_id)\n",
    "\n",
    "                sharpe_ratio = simulation_result.get(\"sharpe_ratio\", None)\n",
    "                if sharpe_ratio is not None and sharpe_ratio > 0.7:\n",
    "                    result = {\n",
    "                        \"alpha_id\": alpha_id,\n",
    "                        \"expression\": alpha,\n",
    "                        \"sharpe_ratio\": sharpe_ratio,\n",
    "                        \"universe\": simulation_result.get(\"universe\"),\n",
    "                        \"region\": simulation_result.get(\"region\"),\n",
    "                        \"turnover\": simulation_result.get(\"turnover\"),\n",
    "                        \"drawdown\": simulation_result.get(\"drawdown\"),\n",
    "                        \"fitness\": simulation_result.get(\"fitness\"),\n",
    "                    }\n",
    "                    current_batch_results.append(result)\n",
    "                    all_results.append(result)\n",
    "                    logging.info(f\"Sharpe Ratio {sharpe_ratio} exceeds threshold for alpha: {alpha}\")\n",
    "                else:\n",
    "                    logging.info(f\"Sharpe Ratio {sharpe_ratio} is below threshold for alpha: {alpha}\")\n",
    "\n",
    "            except RateLimitExceededError as e:\n",
    "                logging.error(f\"Rate limit exceeded. Stopping processing: {e}\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An error occurred while processing {alpha}: {e}\")\n",
    "                continue\n",
    "\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "        # Write batch results to a file\n",
    "        if current_batch_results:\n",
    "            output_filename = f\"batch_results_{i // batch_size + 1}.csv\"\n",
    "            try:\n",
    "                with open(output_filename, mode='w', newline='') as file:\n",
    "                    writer = csv.DictWriter(file, fieldnames=current_batch_results[0].keys())\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(current_batch_results)\n",
    "                logging.info(f\"Batch results saved to {output_filename}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to write batch results to {output_filename}: {e}\")\n",
    "\n",
    "        logging.info(f\"Finished processing batch {i // batch_size + 1}.\")\n",
    "        time.sleep(random.uniform(1, 2))  # Pause to handle rate limits\n",
    "\n",
    "    # Save all results to a consolidated file\n",
    "    if all_results:\n",
    "        all_results_filename = f\"{datetime.now().strftime('%Y%m%d')}_all_results.csv\"\n",
    "        try:\n",
    "            with open(all_results_filename, mode='w', newline='') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=all_results[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(all_results)\n",
    "            logging.info(f\"All valid results saved to {all_results_filename}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save all results to {all_results_filename}: {e}\")\n",
    "\n",
    "BRAINAPIWRAPPER.process_simulations_in_batches4 = process_simulations_in_batches4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63dda9-54f7-405f-b57f-10da9ce58980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN TO INPUT THE FILE AND RETRIEVE RESULTS AND SHARPE\n",
    "filename = \"datafields_EQUITY_ASI_MINVOL1M.csv\"\n",
    "\n",
    "# Call the function to process simulations in batches\n",
    "process_simulations_in_batches4(filename, batch_size=3)  # Adjust batch_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902695e-0df5-4e36-b987-20ae8eebd270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to store results\n",
    "alpha_ids = []  # List to store alpha IDs\n",
    "simulation_results_list = []  # List to store simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ccb5d-af86-4e11-8fe8-ef27606581f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a97730-2a87-44b1-a433-52bfe2289275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the duration for the simulation (5 minutes)\n",
    "duration = 5 * 60  \n",
    "end_time = time.time() + duration  # Calculate the end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72d121-39f6-47fd-a5a3-5321d1e3c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each value in the first column, how does python for loop work? how to loop through each row in dataframe?\n",
    "for index, row in .iterrows():\n",
    "    first_row = row[0]  # Get the first column value\n",
    "    simulation_data = create_simulation_data([first_row])  # Create simulation data for the current value\n",
    "    print(f\"Simulation data created for {first_row}:\")\n",
    "    print(simulation_data)\n",
    "\n",
    "    # Send simulation and check progress\n",
    "    simulation_location_response = s.send_simulation(simulation_data)\n",
    "    print(\"Location Response: \" + simulation_location_response)\n",
    "\n",
    "    alpha_response = s.check_progress_return_alpha(simulation_location_response)\n",
    "    print(\"Alpha Response: \" + str(alpha_response))\n",
    "\n",
    "    # Store the alpha ID in the list\n",
    "    alpha_ids.append(alpha_response)\n",
    "\n",
    "    # Simulate alpha and save results\n",
    "    while time.time() < end_time:\n",
    "        simulation_results = s.simulate_alpha(simulation_data)  # Assuming simulate_alpha is defined\n",
    "        simulation_results_list.append(simulation_results)  # Store results\n",
    "        s.save_results(simulation_results)  # Assuming save_results is defined\n",
    "        time.sleep(15)  # Adjust the sleep time as needed\n",
    "\n",
    "# Add the alpha IDs to the DataFrame\n",
    "first_column_df['alpha_id'] = alpha_ids\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_filename = \"output_with_alpha_ids.csv\"\n",
    "first_column_df.to_csv(output_filename, index=False)\n",
    "print(f\"Alpha IDs saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c386e-aa3f-481c-93b8-d3bfa3ce595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_alpha_list2 =  [{'type': 'REGULAR', 'settings': {'instrumentType': 'EQUITY', 'region': 'USA', 'universe': 'TOP1000', 'delay': 1, 'decay': 20, 'neutralization': 'NONE', 'truncation': 0.08, 'pasteurization': 'ON', 'testPeriod': 'P1Y6M', 'unitHandling': 'VERIFY', 'nanHandling': 'OFF', 'language': 'FASTEXPR', 'visualization': False}, 'regular': 'vec_avg(anl16_medianest_normal)'}, {'type': 'REGULAR', 'settings': {'instrumentType': 'EQUITY', 'region': 'USA', 'universe': 'TOP1000', 'delay': 1, 'decay': 20, 'neutralization': 'NONE', 'truncation': 0.08, 'pasteurization': 'ON', 'testPeriod': 'P1Y6M', 'unitHandling': 'VERIFY', 'nanHandling': 'OFF', 'language': 'FASTEXPR', 'visualization': False}, 'regular': 'vec_avg(anl16_medianrec)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7a3b6-a61f-45bc-9cfc-120f6bf280e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json1 = json_output = json.dumps(truncated_alpha_list2, indent=4)\n",
    "print (json1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971fc5f-c6f8-400e-b230-4642acb2c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate_alpha > send_simulation > check_progress_return_alpha > get_results_dictionary & save_results\n",
    "\n",
    "while time.time() < end_time:\n",
    "    for alpha1 in simulation_data2:\n",
    "        print(\"ALL:\" + str(alpha1))\n",
    "\n",
    "#        simulation_settings=json1\n",
    "        simulation_results = s.simulate_alpha(simulation_data2)\n",
    "        s.save_results(simulation_results)\n",
    "        time.sleep(15)  # Adjust the sleep time as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31154b-8247-4aa7-acbd-2345f026c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = s.session.get(\"https://api.worldquantbrain.com/alphas/\" + alpha_response + \"/recordsets/pnl\").json()\n",
    "print(response1)\n",
    "response2 = s.session.get(\"https://api.worldquantbrain.com/alphas/\" + alpha_response + \"/recordsets/sharpe\").json()\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d1f12-c5c6-41d7-81f5-8d8b72d5404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_datafields1(\n",
    "    self,\n",
    "    instrument_type: str = 'EQUITY',\n",
    "    region: str = 'JPN',\n",
    "    delay: int = 1,\n",
    "    universe: str = 'TOP1600',\n",
    "    dataset_id: str = '',\n",
    "    search: str = '',\n",
    "#    batch_size: int = 10  # Specify the batch size\n",
    "    batch_size: int = 10\n",
    "):\n",
    "    offset = 0\n",
    "    total_count = 0\n",
    "    all_results = []\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL for the API request\n",
    "        url_template = brain_api_url + \"/data-fields?\" +\\\n",
    "            f\"&instrumentType={instrument_type}\" +\\\n",
    "            f\"&region={region}&delay={str(delay)}&universe={universe}&limit={batch_size}\" +\\\n",
    "            f\"&offset={offset}\"\n",
    "\n",
    "        # Attempt to retrieve data\n",
    "        for _ in range(5):  # Retry up to 5 times\n",
    "            response = self.session.get(url_template)\n",
    "            data = response.json()\n",
    "            if 'results' in data:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(5)\n",
    "\n",
    "        # Check if there are results\n",
    "        results = data.get('results', [])\n",
    "        if not results:\n",
    "            print (\"No results.\")\n",
    "            break  # Exit the loop if no more results\n",
    "\n",
    "        # Highlighted Change: Print the retrieved results for this batch\n",
    "        print(f\"Retrieved batch of {len(results)} datafields:\")  # New line added\n",
    "        for index, datafield in enumerate(results):  # New loop added\n",
    "            print(f\"Datafield {index + 1}: {datafield}\")  # New line added\n",
    "        \n",
    "        all_results.extend(results)  # Collect all results\n",
    "        offset += batch_size\n",
    "        total_count += len(results)\n",
    "\n",
    "        # Optional: Print progress\n",
    "        print(f'Retrieved {total_count} datafields so far...')\n",
    "\n",
    "    print(f'Total datafields retrieved: {total_count}')\n",
    "    return all_results  # Return all collected results\n",
    "\n",
    "BRAINAPIWRAPPER.retrieve_datafields1 = retrieve_datafields1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47439f1b-44b8-413e-a311-1e6fe62b30db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aced37c-fe53-4356-a2e6-4232ab58421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = s.retrieve_datafields1(batch_size=10)\n",
    "\n",
    "# Check if any data fields were retrieved\n",
    "if datafields:\n",
    "    # Print each retrieved data field\n",
    "    for index, datafield in enumerate(datafields):\n",
    "        print(f\"Datafield {index + 1}: {datafield}\")\n",
    "else:\n",
    "    print(\"No data fields retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc7edb-f1e1-47f2-885d-2e130eff74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate method for get vector datafield, matrix datafields, others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bde91c-5417-446c-a7d0-33e9ecd1345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datafields_to_csv_file(datafields, output_file='datafields.csv'):\n",
    "    # Convert results to DataFrame\n",
    "    datafields_df = pd.DataFrame(datafields)\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    datafields_df.to_csv(output_file, mode='w', header=True, index=False)  # Write with header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363de3f8-da54-4ba6-9dda-32bf7c7388be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sharpe_ratio (self, alpha_response):\n",
    "\n",
    "    self.session.get(\"https://api.worldquantbrain.com/alphas/\" + alpha_response + \"/recordsets/pnl\").json()\n",
    "    response = self.session.get(\"https://api.worldquantbrain.com/alphas/\" + alpha_response + \"/recordsets/pnl\")\n",
    "    print(\"Status Code:\", response.status_code)\n",
    "    if response.text:\n",
    "        data = response.json()\n",
    "    else:\n",
    "        print(\"Error: Empty response body.\")\n",
    "    print(\"Response Headers:\", response.headers)\n",
    "\n",
    "    sharpe_ratio = None\n",
    "    for check in alpha_checks['is']['checks']:\n",
    "        if check['name'] == 'LOW_SHARPE':\n",
    "            sharpe_ratio = check['value']\n",
    "            break\n",
    "\n",
    "    # Output the Sharpe ratio\n",
    "    if sharpe_ratio is not None:\n",
    "        print(\"Sharpe Ratio (from LOW_SHARPE check):\", sharpe_ratio)\n",
    "    else:\n",
    "        print(\"Sharpe Ratio not found.\")\n",
    "\n",
    "    return sharpe_ratio\n",
    "\n",
    "BRAINAPIWRAPPER.get_sharpe_ratio = get_sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af2b4f-d6a8-4096-8d17-bd0d4d52ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratio_response = s.get_sharpe_ratio(alpha_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0950da-bc56-49d2-9f8a-4f454123b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(self, instrument_type: str = 'EQUITY', region: str = 'USA', delay: int = 1, universe: str = 'TOP3000', file_path: str = None):\n",
    "    url = brain_api_url + \"/data-sets?\" +\\\n",
    "        f\"instrumentType={instrument_type}&region={region}&delay={str(delay)}&universe={universe}\"\n",
    "    result = self.session.get(url)\n",
    "    datasets_df = pd.DataFrame(result.json()['results'])\n",
    "    \n",
    "    # Save the DataFrame to a CSV file if a file path is provided\n",
    "    if file_path:\n",
    "        datasets_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    return datasets_df\n",
    "\n",
    "# Add the new method to the BRAINAPIWRAPPER class\n",
    "BRAINAPIWRAPPER.get_datasets = get_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17a035-a42e-49b8-b57e-2678ab6722a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87776e03-b2ce-49a4-9bb5-420fc4041e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65295e19-3d4e-4d90-9bde-c1ebb21867b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Notes: check for matrix, ask chatgpt for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598b7b4-9030-4c47-96b6-8d22677890a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011f9e5-c4cc-426e-9d06-cbf98464b64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
