{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54221cc-7633-459e-b29d-4076b6818b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "import time\n",
    "import ast\n",
    "import json\n",
    "\n",
    "brain_api_url = os.environ.get(\"BRAIN_API_URL\", \"https://api.worldquantbrain.com\")\n",
    "brain_url = os.environ.get(\"BRAIN_URL\", \"https://platform.worldquantbrain.com\") \n",
    "\n",
    "\n",
    "class BRAINAPIWRAPPER:\n",
    "    \n",
    "    def __init__(self, id_file='retrieved_ids.json'):\n",
    "        self.session = self.get_login_session()\n",
    "        self.permissions = self.check_permissions()  # Initialize permissions here\n",
    "        self.id_file = id_file\n",
    "        self.retrieved_ids = self.load_retrieved_ids()  # Load existing IDs from the file\n",
    "\n",
    "        \n",
    "    def get_login_session(self):\n",
    "        session = requests.Session()\n",
    "        username = os.getenv('wqbrain_consultant_user')\n",
    "        password = os.getenv('wqbrain_consultant_pw')\n",
    "        session.auth = (username, password)\n",
    "        response = session.post('https://api.worldquantbrain.com/authentication')\n",
    "        response.headers\n",
    "        print(username)\n",
    "\n",
    "        if response.status_code == requests.status_codes.codes.unauthorized:\n",
    "            if response.headers[\"WWW-Authenticate\"] == \"persona\":\n",
    "                biometric_url = urljoin(response.url, response.headers[\"Location\"])\n",
    "                print(biometric_url)\n",
    "                input(\"Complete bio\" + biometric_url)\n",
    "                biometric_response = session.post(biometric_url)\n",
    "        else:\n",
    "            print(\"incorrect\")\n",
    "        return session\n",
    "    \n",
    "    def check_permissions(self):\n",
    "        response = self.session.get('https://api.worldquantbrain.com/authentication')\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            permissions = data.get('permissions', [])\n",
    "            print(\"User permissions: \", permissions)\n",
    "            return permissions\n",
    "        else:\n",
    "            print(\"Failed to retrieve permissions: \", response.status_code)\n",
    "            return []\n",
    "    \n",
    "    def has_multi_simulation_permission(self):\n",
    "        return \"MULTI_SIMULATION\" in self.permissions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d3b52-d485-415b-b199-6805a8c356e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store your WorldQuant login in a .env file, in the same folder so above reads in.\n",
    "# wqbrain_consultant_user=xxx@mail.com\n",
    "# wqbrain_consultant_pw=password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47439f1b-44b8-413e-a311-1e6fe62b30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retrieved_ids(self):\n",
    "    \"\"\"Load retrieved IDs from a JSON file.\"\"\"\n",
    "    if os.path.exists(self.id_file):\n",
    "        with open(self.id_file, 'r') as f:\n",
    "            return set(json.load(f))  # Load IDs into a set for fast lookup\n",
    "    return set()  # Return an empty set if the file does not exist\n",
    "\n",
    "BRAINAPIWRAPPER.load_retrieved_ids = load_retrieved_ids\n",
    "\n",
    "\n",
    "def save_retrieved_ids(self):\n",
    "    \"\"\"Save retrieved IDs to a JSON file.\"\"\"\n",
    "    with open(self.id_file, 'w') as f:\n",
    "        json.dump(list(self.retrieved_ids), f)  # Save IDs as a list\n",
    "\n",
    "BRAINAPIWRAPPER.save_retrieved_ids = save_retrieved_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942b3b75-2064-457d-b791-457965eedb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chase78@gmail.com\n",
      "https://api.worldquantbrain.com/authentication/persona?inquiry=inq_JXY4NS1K4VXPr6UfqcEArNrMPTAP\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Complete biohttps://api.worldquantbrain.com/authentication/persona?inquiry=inq_JXY4NS1K4VXPr6UfqcEArNrMPTAP \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User permissions:  ['CONSULTANT', 'MULTI_SIMULATION', 'PROD_ALPHAS', 'REFERRAL', 'VISUALIZATION', 'WORKDAY']\n"
     ]
    }
   ],
   "source": [
    "s = BRAINAPIWRAPPER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f20a2c-8073-43ac-b17b-17810d94699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_save_datafields(\n",
    "    self,\n",
    "    instrument_type: str = 'EQUITY',\n",
    "    region: str = 'GLB',\n",
    "    delay: int = 1,\n",
    "    universe: str = 'MINVOL1M',\n",
    "    dataset_id: str = '',\n",
    "    search: str = '',\n",
    "    batch_size: int = None\n",
    "):\n",
    "    if len(search) == 0:\n",
    "        url_template = brain_api_url + \"/data-fields?\" +\\\n",
    "            f\"&instrumentType={instrument_type}\" +\\\n",
    "            f\"&region={region}&delay={str(delay)}&universe={universe}&dataset.id={dataset_id}&limit=50\" +\\\n",
    "            \"&offset={x}\"\n",
    "        count = self.session.get(url_template.format(x=0)).json()['count'] \n",
    "    else:\n",
    "        url_template = brain_api_url + \"/data-fields?\" +\\\n",
    "            f\"&instrumentType={instrument_type}\" +\\\n",
    "            f\"&region={region}&delay={str(delay)}&universe={universe}&limit=50\" +\\\n",
    "            f\"&search={search}\" +\\\n",
    "            \"&offset={x}\"\n",
    "        count = 100\n",
    "    \n",
    "    max_try = 5\n",
    "    datafields_list = []\n",
    "    for x in range(0, count, 50):\n",
    "        for _ in range(max_try):\n",
    "            datafields = self.session.get(url_template.format(x=x))\n",
    "            if 'results' in datafields.json():\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(5)\n",
    "            \n",
    "        datafields_list.append(datafields.json()['results'])\n",
    "\n",
    "    datafields_list_flat = [item for sublist in datafields_list for item in sublist]\n",
    "\n",
    "    datafields_df = pd.DataFrame(datafields_list_flat)\n",
    "    print(datafields_df)    \n",
    "\n",
    "    # Generate a filename based on parameters\n",
    "    filename = f\"datafields_{instrument_type}_{region}_{universe}.csv\"\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    datafields_df.to_csv(filename, mode='w', header=True, index=False)  # Write with header\n",
    "    print(f\"Data fields saved to {filename}\")\n",
    "\n",
    "    return datafields_df\n",
    "\n",
    "# Add the new method to the BRAINAPIWRAPPER class\n",
    "BRAINAPIWRAPPER.get_and_save_datafields = get_and_save_datafields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81769dca-7ac2-4ab2-ba91-6f9915def94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     id  \\\n",
      "0                                                 adv20   \n",
      "1                                            anl11_2_1e   \n",
      "2                                            anl11_2_1g   \n",
      "3                                          anl11_2_1pme   \n",
      "4                                          anl11_2_1tic   \n",
      "...                                                 ...   \n",
      "9995  oth455_customer_n2v_p10_q50_w2_pca_fact3_clust...   \n",
      "9996  oth455_customer_n2v_p10_q50_w2_pca_fact3_clust...   \n",
      "9997     oth455_customer_n2v_p10_q50_w2_pca_fact3_value   \n",
      "9998   oth455_customer_n2v_p10_q50_w3_kmeans_cluster_10   \n",
      "9999   oth455_customer_n2v_p10_q50_w3_kmeans_cluster_20   \n",
      "\n",
      "                                            description  \\\n",
      "0                  Average daily volume in past 20 days   \n",
      "1     Aggregate KPI for Pollution Prevention & Envir...   \n",
      "2     Aggregate KPI for Board Independence, Diversit...   \n",
      "3     Aggregate KPI for Compensation & Employee Sati...   \n",
      "4     Aggregate KPI for Community Engagement & Chari...   \n",
      "...                                                 ...   \n",
      "9995  Grouping data. Embedded using N2V with custome...   \n",
      "9996  Grouping data. Embedded using N2V with custome...   \n",
      "9997  The 3rd eigenvalue of PCA, embedded using N2V ...   \n",
      "9998  Grouping data. Embedded using N2V with custome...   \n",
      "9999  Grouping data. Embedded using N2V with custome...   \n",
      "\n",
      "                                                dataset  \\\n",
      "0     {'id': 'pv1', 'name': 'Price Volume Data for E...   \n",
      "1             {'id': 'analyst11', 'name': 'ESG scores'}   \n",
      "2             {'id': 'analyst11', 'name': 'ESG scores'}   \n",
      "3             {'id': 'analyst11', 'name': 'ESG scores'}   \n",
      "4             {'id': 'analyst11', 'name': 'ESG scores'}   \n",
      "...                                                 ...   \n",
      "9995  {'id': 'other455', 'name': 'Relationship enhan...   \n",
      "9996  {'id': 'other455', 'name': 'Relationship enhan...   \n",
      "9997  {'id': 'other455', 'name': 'Relationship enhan...   \n",
      "9998  {'id': 'other455', 'name': 'Relationship enhan...   \n",
      "9999  {'id': 'other455', 'name': 'Relationship enhan...   \n",
      "\n",
      "                                  category  \\\n",
      "0     {'id': 'pv', 'name': 'Price Volume'}   \n",
      "1     {'id': 'analyst', 'name': 'Analyst'}   \n",
      "2     {'id': 'analyst', 'name': 'Analyst'}   \n",
      "3     {'id': 'analyst', 'name': 'Analyst'}   \n",
      "4     {'id': 'analyst', 'name': 'Analyst'}   \n",
      "...                                    ...   \n",
      "9995      {'id': 'other', 'name': 'Other'}   \n",
      "9996      {'id': 'other', 'name': 'Other'}   \n",
      "9997      {'id': 'other', 'name': 'Other'}   \n",
      "9998      {'id': 'other', 'name': 'Other'}   \n",
      "9999      {'id': 'other', 'name': 'Other'}   \n",
      "\n",
      "                                            subcategory region  delay  \\\n",
      "0     {'id': 'pv-price-volume', 'name': 'Price Volume'}    GLB      1   \n",
      "1                  {'id': 'analyst-esg', 'name': 'ESG'}    GLB      1   \n",
      "2                  {'id': 'analyst-esg', 'name': 'ESG'}    GLB      1   \n",
      "3                  {'id': 'analyst-esg', 'name': 'ESG'}    GLB      1   \n",
      "4                  {'id': 'analyst-esg', 'name': 'ESG'}    GLB      1   \n",
      "...                                                 ...    ...    ...   \n",
      "9995    {'id': 'other-aiml-data', 'name': 'AI/ML Data'}    GLB      1   \n",
      "9996    {'id': 'other-aiml-data', 'name': 'AI/ML Data'}    GLB      1   \n",
      "9997    {'id': 'other-aiml-data', 'name': 'AI/ML Data'}    GLB      1   \n",
      "9998    {'id': 'other-aiml-data', 'name': 'AI/ML Data'}    GLB      1   \n",
      "9999    {'id': 'other-aiml-data', 'name': 'AI/ML Data'}    GLB      1   \n",
      "\n",
      "      universe    type  coverage  userCount  alphaCount  pyramidMultiplier  \\\n",
      "0     MINVOL1M  MATRIX    1.0000        532       10956                2.0   \n",
      "1     MINVOL1M  MATRIX    0.8216         41          80                2.0   \n",
      "2     MINVOL1M  MATRIX    0.8301         10          18                2.0   \n",
      "3     MINVOL1M  MATRIX    0.8252         19          30                2.0   \n",
      "4     MINVOL1M  MATRIX    0.8345          9          13                2.0   \n",
      "...        ...     ...       ...        ...         ...                ...   \n",
      "9995  MINVOL1M   GROUP    0.7721          1           2                2.0   \n",
      "9996  MINVOL1M   GROUP    0.7618          0           0                2.0   \n",
      "9997  MINVOL1M  MATRIX    0.7735          3           4                2.0   \n",
      "9998  MINVOL1M   GROUP    0.7706          2           2                2.0   \n",
      "9999  MINVOL1M   GROUP    0.7717          2           3                2.0   \n",
      "\n",
      "     themes  \n",
      "0        []  \n",
      "1        []  \n",
      "2        []  \n",
      "3        []  \n",
      "4        []  \n",
      "...     ...  \n",
      "9995     []  \n",
      "9996     []  \n",
      "9997     []  \n",
      "9998     []  \n",
      "9999     []  \n",
      "\n",
      "[10000 rows x 14 columns]\n",
      "Data fields saved to datafields_EQUITY_GLB_MINVOL1M.csv\n",
      "Number of data fields retrieved: 10000\n",
      "s: <__main__.BRAINAPIWRAPPER object at 0x0000027792E79F90>\n"
     ]
    }
   ],
   "source": [
    "all_datafields=s.get_and_save_datafields()\n",
    "print(f\"Number of data fields retrieved: {len(all_datafields)}\")\n",
    "print(f\"s: {s}\")  # Check if s is defined correctly\n",
    "\n",
    "#if all_datafields:\n",
    "#    s.save_datafields_to_csv_file(datafields, output_file='datafieldsJPN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89196217-49b0-4656-8fbb-cf090a6e4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above retrieves 10000 fields sequentially from the 1st field. Please look to improve this. Below are other code used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2bde91c-5417-446c-a7d0-33e9ecd1345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datafields_to_csv_file(datafields):\n",
    "    # Convert results to DataFrame\n",
    "    datafields_df = pd.DataFrame(datafields)\n",
    "\n",
    "    # Write the DataFrame to a CSV file, I REMOVED OUTPUT FILE\n",
    "    datafields_df.to_csv(output_file, mode='w', header=True, index=False)  # Write with header\n",
    "\n",
    "BRAINAPIWRAPPER.save_datafields_to_csv_file = save_datafields_to_csv_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52d1f12-c5c6-41d7-81f5-8d8b72d5404a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_datafields2(\n",
    "    self,\n",
    "    instrument_type: str = 'EQUITY',\n",
    "    region: str = 'GLB',\n",
    "    delay: int = 1,\n",
    "    universe: str = 'MINVOL1M',\n",
    "    dataset_id: str = '',\n",
    "    search: str = '',\n",
    "    batch_size: int = None,  # Remove the default value\n",
    "    max_volume: int = 50   # New parameter to specify the maximum volume\n",
    "):\n",
    "    # Set a default value for batch_size if it is not provided\n",
    "    if batch_size is None:\n",
    "        batch_size = 10  # You can set any default value you prefer\n",
    "\n",
    "    offset = 0\n",
    "    total_count = 0\n",
    "    all_results = []\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL for the API request\n",
    "        url_template = brain_api_url + \"/data-fields?\" +\\\n",
    "            f\"&instrumentType={instrument_type}\" +\\\n",
    "            f\"&region={region}&delay={str(delay)}&universe={universe}&limit={batch_size}\" +\\\n",
    "            f\"&offset={offset}\"\n",
    "\n",
    "        # Attempt to retrieve data\n",
    "        for _ in range(5):  # Retry up to 5 times\n",
    "            response = self.session.get(url_template)\n",
    "            data = response.json()\n",
    "            if 'results' in data:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(5)\n",
    "\n",
    "        # Check if there are results\n",
    "        results = data.get('results', [])\n",
    "        if not results:\n",
    "            print(\"No results.\")\n",
    "            break  # Exit the loop if no more results\n",
    "\n",
    "        # Highlighted Change: Print the retrieved results for this batch\n",
    "        print(f\"Retrieved batch of {len(results)} datafields:\")\n",
    "        new_results = []  # List to store new results that are not already retrieved\n",
    "\n",
    "        for datafield in results:\n",
    "            # Assuming each datafield has a unique identifier, e.g., 'id'\n",
    "            datafield_id = datafield.get('id')  # Adjust this based on your data structure\n",
    "\n",
    "            # Check if this datafield has already been retrieved\n",
    "            if datafield_id not in self.retrieved_ids:\n",
    "                new_results.append(datafield)  # Add to new results\n",
    "                self.retrieved_ids.add(datafield_id)  # Mark as retrieved\n",
    "\n",
    "        # If there are new results, extend the all_results list\n",
    "        if new_results:\n",
    "            all_results.extend(new_results)\n",
    "            total_count += len(new_results)\n",
    "\n",
    "            # Print each new data field\n",
    "            for index, datafield in enumerate(new_results):\n",
    "                print(f\"Datafield {index + 1}: {datafield}\")\n",
    "\n",
    "        # Check if total_count has reached or exceeded max_volume\n",
    "        if total_count >= max_volume:\n",
    "            print(f\"Reached maximum volume of {max_volume} datafields.\")\n",
    "            break  # Exit the loop if the maximum volume is reached\n",
    "\n",
    "\n",
    "        \n",
    "        # Optional: Print progress\n",
    "        print(f'Total datafields retrieved so far: {total_count}')\n",
    "\n",
    "    # Save the retrieved IDs to the file\n",
    "    self.save_retrieved_ids()\n",
    "    print(f'Total datafields retrieved: {total_count}')\n",
    "    return all_results  # Return all collected results\n",
    "\n",
    "\n",
    "BRAINAPIWRAPPER.retrieve_datafields2 = retrieve_datafields2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e075ed-238b-45e1-a470-fd66d19d81f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_datafields3(\n",
    "    self,\n",
    "    instrument_type: str = 'EQUITY',\n",
    "    region: str = 'GLB',\n",
    "    delay: int = 1,\n",
    "    universe: str = 'MINVOL1M',\n",
    "    dataset_id: str = '',\n",
    "    search: str = '',\n",
    "    batch_size: int = None,\n",
    "    max_volume: int = 50\n",
    "):\n",
    "    if batch_size is None:\n",
    "        batch_size = 50\n",
    "\n",
    "    offset = 0\n",
    "    total_count = 0\n",
    "    all_results = []\n",
    "    no_new_data_count = 0  # Counter for no new data\n",
    "\n",
    "    while True:\n",
    "        url_template = brain_api_url + \"/data-fields?\" +\\\n",
    "            f\"&instrumentType={instrument_type}\" +\\\n",
    "            f\"&region={region}&delay={str(delay)}&universe={universe}&limit={batch_size}\" +\\\n",
    "            f\"&offset={offset}\"\n",
    "\n",
    "        for _ in range(5):\n",
    "            response = self.session.get(url_template)\n",
    "            data = response.json()\n",
    "            if 'results' in data:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(5)\n",
    "\n",
    "        results = data.get('results', [])\n",
    "        if not results:\n",
    "            print(\"No results.\")\n",
    "            break\n",
    "\n",
    "#        print(f\"Retrieved batch of {len(results)} datafields:\")\n",
    "        new_results = []\n",
    "\n",
    "        for datafield in results:\n",
    "            datafield_id = datafield.get('id')\n",
    "            if datafield_id not in self.retrieved_ids:\n",
    "                new_results.append(datafield)\n",
    "                self.retrieved_ids.add(datafield_id)\n",
    "\n",
    "        if new_results:\n",
    "            all_results.extend(new_results)\n",
    "            total_count += len(new_results)\n",
    "            no_new_data_count = 0  # Reset the counter\n",
    "\n",
    "            for index, datafield in enumerate(new_results):\n",
    "                print(f\"Datafield {index + 1}: {datafield}\")\n",
    "        else:\n",
    "            no_new_data_count += 1\n",
    "            print(\"No new datafields found in this batch.\")\n",
    "\n",
    "        if total_count >= max_volume:\n",
    "            print(f\"Reached maximum volume of {max_volume} datafields.\")\n",
    "            break\n",
    "\n",
    "        # Break if no new data has been found for a certain number of iterations\n",
    "        if no_new_data_count >= 3:  # Adjust this threshold as needed\n",
    "            print(\"No new datafields found for several iterations. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        print(f'Total datafields retrieved so far: {total_count}')\n",
    "        offset += batch_size  # Increment offset for the next batch\n",
    "\n",
    "    self.save_retrieved_ids()\n",
    "    print(f'Total datafields retrieved: {total_count}')\n",
    "    return all_results\n",
    "\n",
    "BRAINAPIWRAPPER.retrieve_datafields3 = retrieve_datafields3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "665af85d-a1fe-4690-a21f-531adcf8b7e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_datafields(\n",
    "    self,\n",
    "    instrument_type: str = 'EQUITY',\n",
    "    region: str = 'GLB',\n",
    "    delay: int = 1,\n",
    "    universe: str = 'MINVOL1M',\n",
    "    dataset_id: str = '',\n",
    "    search: str = '',\n",
    "    batch_size: int = None\n",
    "#   limit: int = 20  # Add a limit parameter with a default value\n",
    "):\n",
    "    if len(search) == 0:\n",
    "        url_template = brain_api_url + \"/data-fields?\" +\\\n",
    "            f\"&instrumentType={instrument_type}\" +\\\n",
    "            f\"&region={region}&delay={str(delay)}&universe={universe}&dataset.id={dataset_id}&limit=50\" +\\\n",
    "            \"&offset={x}\"\n",
    "        count = self.session.get(url_template.format(x=0)).json()['count'] \n",
    "    else:\n",
    "        url_template = brain_api_url + \"/data-fields?\" +\\\n",
    "            f\"&instrumentType={instrument_type}\" +\\\n",
    "            f\"&region={region}&delay={str(delay)}&universe={universe}&limit=50\" +\\\n",
    "            f\"&search={search}\" +\\\n",
    "            \"&offset={x}\"\n",
    "        count = 100\n",
    "    \n",
    "    max_try=5\n",
    "    datafields_list = []\n",
    "    for x in range(0, count, 50):\n",
    "        for _ in range(max_try):\n",
    "            datafields = self.session.get(url_template.format(x=x))\n",
    "            if 'results' in datafields.json():\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(5)\n",
    "            \n",
    "        datafields_list.append(datafields.json()['results'])\n",
    "\n",
    "    datafields_list_flat = [item for sublist in datafields_list for item in sublist]\n",
    "\n",
    "    datafields_df = pd.DataFrame(datafields_list_flat)\n",
    "    print(datafields_df)    \n",
    "    return datafields_df\n",
    "\n",
    "\n",
    "# Add the new method to the BRAINAPIWRAPPER class\n",
    "BRAINAPIWRAPPER.get_datafields = get_datafields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aced37c-fe53-4356-a2e6-4232ab58421d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BRAINAPIWRAPPER' object has no attribute 'retrieve_datafields3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m datafields \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_datafields3\u001b[49m(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#datafields = s.retrieve_datafields()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of data fields retrieved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(datafields)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BRAINAPIWRAPPER' object has no attribute 'retrieve_datafields3'"
     ]
    }
   ],
   "source": [
    "datafields = s.retrieve_datafields3(batch_size=50)\n",
    "#datafields = s.retrieve_datafields()\n",
    "\n",
    "print(f\"Number of data fields retrieved: {len(datafields)}\")\n",
    "print(f\"s: {s}\")  # Check if s is defined correctly\n",
    "\n",
    "if datafields:\n",
    "    s.save_datafields_to_csv_file(datafields, output_file='datafieldsGLBMINVOL.csv')\n",
    "\n",
    "\n",
    "# Check if any data fields were retrieved\n",
    "if datafields:\n",
    "    # Print each retrieved data field\n",
    "    for index, datafield in enumerate(datafields):\n",
    "        print(f\"Datafield {index + 1}: {datafield}\")\n",
    "else:\n",
    "    print(\"No data fields retrieved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f946172-c3a6-4b16-aa70-d9736bef2577",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "datafields = s.retrieve_datafields(batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc7edb-f1e1-47f2-885d-2e130eff74c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#separate method for get vector datafield, matrix datafields, others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925f5af-33a5-443e-a8d7-b8954978eb1d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0950da-bc56-49d2-9f8a-4f454123b126",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_datasets(self, instrument_type: str = 'EQUITY', region: str = 'USA', delay: int = 1, universe: str = 'TOP3000', file_path: str = None):\n",
    "    url = brain_api_url + \"/data-sets?\" +\\\n",
    "        f\"instrumentType={instrument_type}&region={region}&delay={str(delay)}&universe={universe}\"\n",
    "    result = self.session.get(url)\n",
    "    datasets_df = pd.DataFrame(result.json()['results'])\n",
    "    \n",
    "    # Save the DataFrame to a CSV file if a file path is provided\n",
    "    if file_path:\n",
    "        datasets_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    return datasets_df\n",
    "\n",
    "# Add the new method to the BRAINAPIWRAPPER class\n",
    "BRAINAPIWRAPPER.get_datasets = get_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17a035-a42e-49b8-b57e-2678ab6722a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87776e03-b2ce-49a4-9bb5-420fc4041e8f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598b7b4-9030-4c47-96b6-8d22677890a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011f9e5-c4cc-426e-9d06-cbf98464b64f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd4589-897c-425e-9bfc-22dce7783738",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
